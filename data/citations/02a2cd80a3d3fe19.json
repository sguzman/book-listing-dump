[
  {
    "author": [
      {
        "family": "Codd",
        "given": "E.F."
      }
    ],
    "note": [
      "CIF (corporate information factory),",
      "valid explicit values, 143–144 key constituencies, 117–119 command-line execution, policy, formulating, 122–124 scheduling and support, 306 screens and measurements Common Warehouse Metamodel anomaly detection phase, (CWM), 378"
    ],
    "pages": [
      "144–146",
      "134–135 120–122"
    ],
    "title": [
      "25 valid column explicit values, cleaning 143–144 assumptions, 116–117 closed-loop movement, 305 big dimensions, 174–175 COBOL data and value rule copybooks, extracting, 78–79 enforcement, 135–136 OCCURS clause, 85–87 reasonability, 147",
      "inventor of relational data quality, 18–19, 115–116, 406 algebra), 340 deliverables collecting and documenting, source audit dimension, 128–131 systems, 63 data profiling, 125 columns described, 124–125 distribution reasonability, 146–147 error event table, 125–127 invalid explicit values, 144 described, 113–115 length restriction, 143 design objectives nullity, 140–141 competing factors, 119–120 numeric and data ranges, 141–143 conflicting priorities, balancing, property enforcement"
    ],
    "type": null
  },
  {
    "note": [
      "environment, ETL system security, data, delivering to OLAP cubes,"
    ],
    "pages": [
      "292–294 148–150"
    ],
    "title": [
      "development network traffic, eliminating, 300 aggregated extracts, creating partitioning and parallelizing, mainframe systems, 274 297–298 Unix and Windows systems, retrieving only needed data, 274–276 299 database bulk loader utilities, time, 260–261 speeding inserts with, 276–280 DFSORT mainframe command, database features improving 264–266 performance dimension described, 280–282 adding, 236 ordinal position of jobs, 282–286 cleaning and conforming, troubleshooting"
    ],
    "type": null
  },
  {
    "note": [
      "delimited, processing, 93 throughput, increasing described, 90–91 aggregates, updating fixed length, processing, 91–92 incrementally, 298–299 heterogeneous data sources, logging, 299 logical data map Index 475 business rules, collecting, 73 F components, 58–62 fact tables data content analysis, 71–72 accumulating snapshot, 222–224 data discovery, 63–71 aggregations, 241–247 designing before physical, 56–58 deleting facts, 230 success and, 62 described, 209–212 tools, 62 dimensional data, delivering to mainframe sources OLAP cubes, 247–253"
    ],
    "pages": [
      "391–392"
    ],
    "title": [
      "defining, 461–463 extracting evolution of, 464–465 changed data future of, 463–464 deleted or overwritten, 111 metadata generated by described, 105–106 batch, 373–374 detecting, 106–109 described, 367–368 tips, 109–110 job, 368–370 described, 18, 55–56, 116 transformation, 370–373 disparate platforms, challenge of security extracting from, 76 described, 343–344 diverse sources, connecting development environment, 344 through ODBC, 76–77 production environment, ERP system sources, 102–105 344–345 flat files team"
    ],
    "type": null,
    "volume": [
      "28"
    ]
  },
  {
    "note": [
      "partitions, 224–226 PICtures, using, 81–83 periodic snapshot, 220–222 redefined fields, working with, physically deleting facts, 230–232 84–85 provider, cleaning and variable record lengths, 89–90 conforming, 152 parallel processing queries, referential integrity, 212–214, 285 288–290 revenue, collecting in multiple process time, load time, currencies, 238–239 estimating, 321–323 rollback log, outwitting, 226"
    ],
    "pages": [
      "229–230 83–84"
    ],
    "title": [
      "COBOL copybooks, 78–79 ETL data structure, 45–46 data, transferring between factless, 232–233 platforms, 80–81 fundamental grains, 217–224 described, 78 graceful modifications, 235–236 EBCDIC character set, 79–80 incremental loading, 228 from IMS, IDMS, Adabase, and indexes, 224 Model 204, 90 inserting facts, 228 managing multiple record type late arriving facts, 239–241 files, 87–88 loading data, 226–227 multiple OCCURS, 85–87 logically deleting facts, 232 numeric data, handling, 81 multiple units of measure, 237–238 packed decimals, unpacking, negating facts"
    ],
    "type": null
  },
  {
    "note": [
      "94 updating facts, 230 described, 93–94 facts DTD, 95–96 adding, 236 meta data, described, 94–95 cleaning and conforming, 151–152 namespaces, 97 delivering to OLAP cubes, XML Schema, 96–97 250–252"
    ],
    "title": [
      "Web log sources surrogate key pipeline, 214–217 described, 97–98 transaction grain, 218–220 name value pairs, 100–101 Type 1, augmenting with Type 2 W3C common and extended history, 234–235 formats, 98–100 updating and correcting facts, XML sources 228–229 character sets"
    ],
    "type": null
  },
  {
    "pages": [
      "89–90"
    ],
    "title": [
      "index space, 328 PICtures, using, 81–83 operations redefined fields, working with, data space, 328 84–85 disk failure, 326–327 variable record lengths"
    ],
    "type": null
  },
  {
    "note": [
      "337 described, 377–378 operations establishing, 378–379 CPU usage, 333 naming conventions, 379–380 crucial measurements, 332–333 technical described, 331 business rules, 366–367 disk contention, 336–337 data definitions, 365–366 memory allocation, 333–335 data models, 365 memory contention, 335–336 described, 363–364 processor contention, 337 system inventory, 364–365 server contention, 335 understanding, 39–40 specific performance indicators,"
    ],
    "title": [
      "metadata (cont.) sorting, 266–269 source system information, subset of source file records, 361–362 extracting, 271–272 capturing, 49–51, 309 migrating, long-term archiving and data-staging, 354–355 recovery, 349–350 DBMS, 355–356 migrating to production, operations described, 15, 351–353 described, 315 ETL-generated operational support for data batch, 373–374 warehouse, 316 data quality error event, 374–375 support in production, 319–320 described, 367–368 minitables, 224 job, 368–370 MINUS query, 232 process execution, 375–377 minutes, time dimension, building, transformation, 370–373 173 impact analysis, 49, 380 Model 204, extracting data from, providing and utilizing, 390 90 scheduling and support, modules, cleaning and conforming, 314–315 155–156 source system, 353–354 monitoring standards and practices database contention"
    ],
    "type": null
  },
  {
    "issue": [
      "OLTP"
    ],
    "note": [
      "operational data store (ODS), 20 numeric data operations in descriptive attributes, 163 data warehouse usage, measuring, mainframe sources, handling, 81 337–339"
    ],
    "pages": [
      "432–433,",
      "37–38 140–141"
    ],
    "title": [
      "nibbles, 83 integrating into ETL system, Nissen, Gary (ETL tool versus hand 252–253 coding analysis), 10–11 on-line transaction processing nonevent notification",
      "nonrelational data sources, ETL 442–443 data structure, 42–45 Open Database Connectivity NOT operator, 110 (ODBC) notification and paging, scheduling bottleneck, overcoming, 288 and support, 306–307 diverse sources, connecting, 76–77 NULL values operating system audit columns and, 106–107 scheduling and support, 312–313 described, 71–72 text strings, replacing or text fields, converting to empty, substituting"
    ],
    "type": null,
    "volume": [
      "434"
    ]
  },
  {
    "note": [
      "defined, 162 internal versus external hires, 397 fact table, 211 outsourcing development, process of elimination, changed 400–401 data, detecting, 108–109 recruiters, working with, 396–397 processes team members, selecting, 398 executing, 375–377 team roles and responsibilities, metadata, 50 394–396 tuning, 339–340 tool set, determining, 393–394 Index 485"
    ],
    "pages": [
      "172 398–400"
    ],
    "title": [
      "planning (cont.) processing dimensions, delivering developers, hiring qualified, 387 to OLAP cubes, 248–249 dirty data, handling, 388 processor contention, 337 documentation, maintaining, production environment, ETL 389–390 system security, 344–345 impact analysis, 49 project management large and building small, 385–387 defining project, 392–393 metadata capture, 49–51 described, 391 metadata, providing and utilizing, development environment, 401 390 ETL team responsibility, 391–392 monitoring, auditing, and planning publishing statistics, 389 business requirements analysis, naming conventions, 51 401–405 planning large and building small, data quality strategy, 406–407 385–387 deployment, 410 project management, 393 development environment, 401 simplicity, 390 logical data map, designing, standardization, enforcing, 405–406 388–389 maintaining data warehouse, 412 teams with database expertise, physical ETL process, building, building, 387–388 407–408 throughput, optimizing, 390–391 scope, defining, 405 platforms, transferring data test cases, 409–410, 411 between, 80–81 testing processes, 408–409 point-to-point architecture, 434–436 planning project, 393 policy, data cleaning and scope, managing conforming, 122–124 change tracking, 413–414 populating hierarchy bridge tables, described, 412–413 201–204 version releases, scheduling, positional attributes to represent 414–416 text facts, 204–206 staff primary key building and retaining a team, calendar date dimension"
    ],
    "type": null
  },
  {
    "note": [
      "365 touch points, 428 described, 363–364 TQM (total quality management), system inventory, 364–365 122 temp space, 286, 327–328 tracking, source systems, 63–66 testing transaction grain, fact tables, position of fixed length flat files, 218–220 92 transformation processes, 408–409 ETL-generated metadata, project management, 409–410, 370–373"
    ],
    "title": [
      "team timestamps, 190–192, 439–440 database expertise, 387–388 time-varying bridge tables, members, selecting, 398 dimensions, 198–199 roles and responsibilities, 394–396 token aware function, operations, technical definitions, 50 304–305 technical metadata tool set, determining, project business rules, 366–367 management, 393–394 data definitions, 365–366 tools, logical data map, 62 data models"
    ],
    "type": null
  }
]
